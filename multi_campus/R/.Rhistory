## party 패키지 설치 및 로딩, 패키지 내 데이터 목록조회
#코딩북
#간단 기술통계
data(readingSkills, package = "party")
help(readingSkills)
print(head(readingSkills, n = 10))
ls(readingSkills)
ncol(readingSkills)
nrow(readingSkills)
str(readingSkills)
summary(readingSkills)
## 샘플링 재현성을 위한 초기 시드넘버 생성
myseed <- 100
set.seed(myseed)
## 학습(트레이닝) & 검증(테스트) 데이터 70:30 비율로 추출
trainRowNumbers <- createDataPartition(readingSkills$nativeSpeaker,
p = 0.7, list = FALSE)
trainRowNumbers
## 훈련데이터셋 생성
trainData <- readingSkills[trainRowNumbers, ]
trainData
addmargins(table(trainData$nativeSpeaker))
## 테스트 데이터셋 생성
testData <- readingSkills[-trainRowNumbers, ]
testData
## 훈련데이터와 검증데이터의 예측변수와 반응변수 별도 객체 저장
x.train  =  trainData[, 2:4]
y.train  =  trainData$nativeSpeaker
x.train
y.train
x.test  =  testData[, 2:4]
y.test  =  testData$nativeSpeaker
library(skimr)
skim(trainData)
skim_to_wide(trainData)
psych::describe(trainData)
Hmisc::describe(trainData)
## trainControl() 함수의 예측모델 훈련방법 옵션설정
library(rpart)
set.seed(1234)
## 고전적 의사결정나무 분석실시
rpart.fit <- rpart(y.train ~ ., data = x.train, method = "class",
parms=list(split="information"))
rpart.fit
print(rpart.fit, digits = 2)
plot(rpart.fit, margin= 0.1)
text(rpart.fit, all=TRUE, use.n = TRUE)
plot(rpart.fit, uniform=TRUE, branch=0.6, margin=0.1)
text(rpart.fit, all=TRUE, use.n = TRUE)
summary(rpart.fit)
library(caret)
varImp(rpart.fit)
printcp(rpart.fit)
rpart.fit$cptable
rpart.fit$cptable
min(rpart.fit$cptable[,"xerror"])
which.min(churn.rp$cptable[,"xerror"])
rpart.fit
print(rpart.fit, digits = 2)
plot(rpart.fit, margin= 0.1)
text(rpart.fit, all=TRUE, use.n = TRUE)
plot(rpart.fit, uniform=TRUE, branch=0.6, margin=0.1)
text(rpart.fit, all=TRUE, use.n = TRUE)
summary(rpart.fit)
library(caret)
varImp(rpart.fit)
printcp(rpart.fit)
rpart.fit$cptable
min(rpart.fit$cptable[,"xerror"])
which.min(churn.rp$cptable[,"xerror"])
forest.fit <- randomForest(y.train ~ ., data = x.train,
na.action = na.roughfix,
importance = TRUE)
forest.fit
plot(forest.fit)
importance(forest.fit)
importance(forest.fit, type=1)
importance(forest.fit, type=2)
varImpPlot(forest.fit)
# install.packages(caret)
library(caret)
varImp(forest.fit)
forest.pred.train <- predict(forest.fit, x.train)
forest.pred.train
forest.perf.train <- table(forest.pred.train, df.train$result,
dnn=c("TrainRule", "TrainActual"))
forest.perf.train <- table(forest.pred.train, x.train$result,
dnn=c("TrainRule", "TrainActual"))
forest.perf.train <- table(forest.pred.train, x.train$result,
dnn=c("TrainRule", "TrainActual"))
forest.perf.train <- table(forest.pred.train, y.train,
dnn=c("TrainRule", "TrainActual"))
forest.perf.train
addmargins(table(y.train))
addmargins(forest.perf.train)
sum(forest.perf.train)
sum(diag(forest.perf.train))
sum(diag(forest.perf.train))/sum(forest.perf.train)
forest.pred.test <- predict(forest.fit, x.test)
forest.pred.test
## 검증데이터의 실제 우량/불량 패턴과 분류규칙을 통한 분류패턴간 교차분석
forest.perf.test <- table(forest.pred.test, y.test,
dnn=c("TestPredicted", "TestActual"))
forest.perf.test
addmargins(table(y.test))
addmargins(forest.perf.test)
## 검증(test)데이터 분류결과 정확성(Accuracy) 평가
sum(forest.perf.test)
sum(diag(forest.perf.test))
sum(diag(forest.perf.test))/sum(forest.perf.test)
# 랜덤포레스트 분류모델의
# 학습(tain)데이터와 검증(test)데이터 분류정확성 비교
x <- sum(diag(forest.perf.train))/sum(forest.perf.train)
y <- sum(diag(forest.perf.test))/sum(forest.perf.test)
compare <- c(x, y, y - x)
names(compare) <- c("trainAccuracy", "testAccuracy", "AccuracyGap")
compare
round(compare*100, 2)
## 랜덤포레스트 모델 ROC(Receiver Operating Characteristic) curve 그래프
install.packages("ROCR")
library(ROCR)
forest.pred.score <- predict(forest.fit, df.test, type="prob")
forest.pred.score <- predict(forest.fit, x.test, type="prob")
forest.pred.score
forest.pred.roc <- prediction(forest.pred.score[, 2], y.test)
forest.pred.roc
par(mfrow = c(1, 4))
plot(performance(forest.pred.roc, "tpr", "fpr"),
main="랜덤포레스트를 이용한 ROC커브")
forest.pred.roc
par(mfrow = c(1, 4))
plot(performance(forest.pred.roc, "tpr", "fpr"),
main="랜덤포레스트를 이용한 ROC커브")
abline(0,1, col="red")
auc <- performance(forest.pred.roc,"auc")
auc <- unlist(slot(auc, "y.values"))
auc <- round(auc, 3)
legend(0.6, 0.4, legend=c(paste0("AUC: ",auc)),
cex=1, bty = "n", box.col = "white")
plot(performance(forest.pred.roc, "lift", "rpp"),
main="랜덤포레스트를 이용한 Lift chart 커브")
plot(performance(forest.pred.roc, "prec", "rec"),
main="랜덤포레스트를 이용한 Precision/Recall커브")
plot(performance(forest.pred.roc, "acc", "cutoff"),
main="랜덤포레스트를 이용한 Accuracy/Cutoff커브")
par(mfrow = c(1, 1))
cat("랜덤포레스트 AUC 면적:",
attr(performance(forest.pred.roc, "auc"), "y.values")[[1]])
View(trainData)
## party 패키지 설치 및 로딩, 패키지 내 데이터 목록조회
#install.packages("party")
library(party)
data(package = "party")
library(datasets)
#데이터로드
data(mtcars)
data(mtcars , package = "datasets")
help(mtcars , package = "datasets")
library(arules)
library(arulesSequences)
data("Groceries")
## datasets 패키지 내 mtcars 데이터셋 로딩과 코딩북 확인
data(Groceries , package = "arules")
help(Groceries , package = "arules")
## party 패키지 설치 및 로딩, 패키지 내 데이터 목록조회
#install.packages("party")
library(party)
data(package = "party")
## part 패키지 내 readingSkills 데이터셋 로딩과 코딩북 확인
data(readingSkills , package = "party")
help(readingSkills , package = "party")
## readingSkills 데이터셋에 대한 간단조회, 구조파악, 간단 기술통계분석
summary(readingSkills)
str(readingSkills)
skim(readingSkills)
psych::describe(readingSkills)
Hmisc::describe(readingSkills)
## 반응변수인 nativeSpeaker의 레이블순서를 yes < no 순서로 변경
readingSkills$nativeSpeaker <- factor(c(readingSkills$nativeSpeaker), levels=c(1,2), labels=c("yes", "no"))
## 학습(트레이닝) & 검증(테스트) 데이터 70:30 비율로 추출
myseed <- 100
set.seed(myseed)
## 훈련데이터 인덱스 무작위추출
install.packages("caret")
library(caret)
trainRowNumbers <- createDataPartition(readingSkills$nativeSpeaker,
p = 0.7, list = FALSE)
trainRowNumbers
## 훈련데이터셋 생성
trainData <- readingSkills[trainRowNumbers, ]
trainData
## 테스트 데이터셋 생성
testData <- readingSkills[-trainRowNumbers, ]
testData
x.train  =  trainData[, 2:4]
y.train  =  trainData$nativeSpeaker
x.train
y.train
x.test  =  testData[, 2:4]
y.test  =  testData$nativeSpeaker
x.test
y.test
## 학습 & 검증 데이터 간단조회
Hmisc::describe(x.train)
Hmisc::describe(y.train)
Hmisc::describe(x.test)
Hmisc::describe(y.test)
## 학습데이터를 이용한 분류규칙 생성
library(rpart)
set.seed(1234)
install.packages("randomForest")
library(randomForest)
forest.fit <- randomForest(y.train ~ ., data = x.train,
na.action = na.roughfix,
importance = TRUE)
forest.fit
plot(forest.fit)
importance(forest.fit)
importance(forest.fit, type=1)
importance(forest.fit, type=2)
varImpPlot(forest.fit)
varImp(forest.fit)
## 분류규칙을 이용한 학습(train)데이터 분류분석
forest.pred.train <- predict(forest.fit, x.train)
forest.pred.train
forest.perf.train <- table(forest.pred.train, y.train,
dnn=c("TrainRule", "TrainActual"))
forest.perf.train
addmargins(table(y.train))
addmargins(forest.perf.train)
sum(forest.perf.train)
sum(diag(forest.perf.train))
sum(diag(forest.perf.train))/sum(forest.perf.train)
## 검증(test)데이터에 대한 분류분석
forest.pred.test <- predict(forest.fit, x.test)
forest.pred.test
forest.perf.test <- table(forest.pred.test, y.test,
dnn=c("TestPredicted", "TestActual"))
forest.perf.test
addmargins(table(y.test))
addmargins(forest.perf.test)
sum(forest.perf.test)
sum(diag(forest.perf.test))
sum(diag(forest.perf.test))/sum(forest.perf.test)
x <- sum(diag(forest.perf.train))/sum(forest.perf.train)
y <- sum(diag(forest.perf.test))/sum(forest.perf.test)
compare <- c(x, y, y - x)
names(compare) <- c("trainAccuracy", "testAccuracy", "AccuracyGap")
compare
round(compare*100, 2)
install.packages("e1071")
library(e1071)
library(caret)
confusionMatrix(forest.perf.test, positive = "yes")
